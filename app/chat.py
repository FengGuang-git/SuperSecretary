import os
import json
from openai import OpenAI
import importlib
import pkgutil
import inspect
from types import ModuleType
from typing import List, Tuple, Iterator
import atexit
import tiktoken
from app.mcp.mcp_remote import MCPBridge
import re
import dataclasses
from enum import Enum
import os, json
from pathlib import Path
from dotenv import load_dotenv

# ========== é…ç½® ==========
MAX_TOOL_ROUNDS = 1000

# æ ¹æ® deepseek-reasoner API æ–‡æ¡£è°ƒæ•´æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä»…ç”¨äºæç¤ºï¼Œä¸åšè‡ªåŠ¨ä¿®å‰ªï¼‰
MAX_CONTEXT_LENGTH = 65536  # 64K tokens
TOKEN_BUFFER = 2048
ENCODER_NAME = "cl100k_base"

_DECODER = json.JSONDecoder()

def _json_default(o):
    if hasattr(o, "model_dump"):           # Pydantic v2
        return o.model_dump()
    if hasattr(o, "dict"):                 # Pydantic v1
        return o.dict()
    if dataclasses.is_dataclass(o):
        return dataclasses.asdict(o)
    if hasattr(o, "to_dict"):
        return o.to_dict()
    if isinstance(o, Enum):
        return o.value
    if hasattr(o, "__dict__"):             # æ™®é€šå¯¹è±¡å…œåº•
        return {k: v for k, v in o.__dict__.items()
                if not callable(v) and not k.startswith("_")}
    return str(o)

# ========== è½»é‡å‹ç¼©ï¼ˆä¸æ”¹å˜è¯­ä¹‰ï¼Œä¸æˆªæ–­ï¼‰ ==========
def _minify_json_str(s: str) -> str:
    """å°½é‡æŠŠJSONå­—ç¬¦ä¸²å‹æˆæœ€çŸ­ï¼ˆå»ç©ºæ ¼/æ¢è¡Œï¼‰ï¼Œå¤±è´¥åˆ™åŸæ ·è¿”å›ã€‚"""
    try:
        obj = json.loads(s)
        return json.dumps(obj, ensure_ascii=False, separators=(",", ":"))
    except Exception:
        return s

def _squeeze_text(s: str) -> str:
    """å¯¹éJSONæ–‡æœ¬è¿›è¡Œè½»é‡å‹ç¼©ï¼šå»ä¸¤ç«¯ç©ºç™½ã€æŠ˜å å¤šä½™æ¢è¡Œä¸ç©ºæ ¼ï¼ˆä¸æˆªæ–­ã€ä¸æ”¹è¯­ä¹‰ï¼‰ã€‚"""
    if not isinstance(s, str):
        return s
    s = s.strip()
    # è¿ç»­æ¢è¡Œå‹æˆå•ä¸ªæ¢è¡Œ
    s = re.sub(r"\n{2,}", "\n", s)
    # æ¯è¡Œå†…éƒ¨å¤šç©ºæ ¼æŠ˜å ä¸ºå•ç©ºæ ¼ï¼ˆä¸è§¦ç¢°ä»£ç å—è¯­ä¹‰ï¼‰
    lines = []
    for line in s.split("\n"):
        if line.startswith("    ") or line.startswith("\t") or line.strip().startswith("```"):
            lines.append(line.rstrip())
        else:
            lines.append(re.sub(r"[ \t]{2,}", " ", line).rstrip())
    return "\n".join(lines)

def _is_json_like(s: str) -> bool:
    if not isinstance(s, str):
        return False
    s2 = s.lstrip()
    return s2.startswith("{") or s2.startswith("[")

BASE_DIR = Path(__file__).resolve().parents[1]  # é¡¹ç›®æ ¹ç›®å½• SuperSecretary/
CONFIG_PATH = Path(os.getenv("CONFIG_PATH", BASE_DIR / "config.json"))


class Client:
    def __init__(self):
        # åŠ è½½.envæ–‡ä»¶çš„ç¯å¢ƒå˜é‡
        load_dotenv()
        
        with open(CONFIG_PATH, 'r', encoding='utf-8') as file:
            self.config = json.load(file)

        # ä¿å­˜è¿œç¨‹ MCP æ¡¥æ¥å®ä¾‹ï¼Œä¿è¯å…¨å±€åªå¯ä¸€æ¬¡
        self._mcp_bridges = []
        self.tools, self.funcDict = self.get_mcp_tools()
        # åˆå§‹åŒ–ç¼–ç å™¨
        self.encoder = tiktoken.get_encoding(ENCODER_NAME)

        # é€€å‡ºæ—¶æ¸…ç†
        atexit.register(self._shutdown_mcp)

    # ========== JSON è§£æå·¥å…· ==========
    def _iter_json_objects(self, s: str):
        """ä»å­—ç¬¦ä¸²ä¸­ä¾æ¬¡æå–ä¸€ä¸ªæˆ–å¤šä¸ª JSON å¯¹è±¡/æ•°ç»„ï¼ˆå®¹é”™ä¸²æµ/NDJSONï¼‰ã€‚"""
        if not isinstance(s, str):
            yield s
            return
        i, n = 0, len(s)
        while i < n:
            # è·³è¿‡ç©ºç™½
            while i < n and s[i].isspace():
                i += 1
            if i >= n:
                break
            try:
                obj, end = _DECODER.raw_decode(s, i)
                yield obj
                i = end
            except json.JSONDecodeError:
                j1 = s.find('{', i + 1)
                j2 = s.find('[', i + 1)
                cand = [x for x in (j1, j2) if x != -1]
                if not cand:
                    break
                i = min(cand)

    def safe_json_loads(self, s: str):
        """è¿”å›æœ€åä¸€ä¸ªåˆæ³• JSONï¼ˆé€šå¸¸æ˜¯æ¨¡å‹æœ€ç»ˆè¦†å†™çš„å‚æ•°ï¼‰ã€‚"""
        last = None
        for obj in self._iter_json_objects(s):
            last = obj
        if last is None:
            return {}
        if not isinstance(last, dict):
            if isinstance(last, list) and last and isinstance(last[0], dict):
                return last[0]
            return {"_value": last}
        return last

    # ========== æ¨¡å‹ä¸æ¶ˆæ¯ ==========
    def set_model(self, model):
        self.model = model
        # å¤„ç†ç¯å¢ƒå˜é‡æ›¿æ¢
        api_key = os.getenv("OPENAI_API_KEY", "") if "${OPENAI_API_KEY}" in self.model["key"] else self.model["key"]
        base_url = os.getenv("OPENAI_BASE_URL", "") if "${OPENAI_BASE_URL}" in self.model["url"] else self.model["url"]
        print("==DEBUG== model.name:", self.model.get("name"), "url:", base_url)
        
        self.instance = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        print("è®¾ç½®æ¨¡å‹ï¼š" + self.model["name"])
        self.rest_message()

    def rest_message(self):
        self.messages = [{"role": "system", "content": '\n'.join(self.config["main_prompts"])}]
        print("æ¶ˆæ¯å·²ç»é‡ç½®")

    def _count_tokens(self, messages):
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡ï¼ˆä»…ç”¨äºæç¤ºï¼Œä¸åšè‡ªåŠ¨ä¿®å‰ªï¼‰"""
        total_tokens = 0
        for message in messages:
            for key, value in message.items():
                if isinstance(value, str):
                    total_tokens += len(self.encoder.encode(value))
                elif isinstance(value, dict):
                    total_tokens += len(self.encoder.encode(json.dumps(value, ensure_ascii=False)))
                elif isinstance(value, list):
                    total_tokens += len(self.encoder.encode(json.dumps(value, ensure_ascii=False)))
        return total_tokens

    def _trim_context(self):
        """ç¦ç”¨ä¸Šä¸‹æ–‡ä¿®å‰ªï¼šä¸åšä»»ä½•äº‹ï¼Œç¡®ä¿å‘ç»™æ¨¡å‹çš„æ¶ˆæ¯å®Œå…¨ä¿ç•™ã€‚"""
        return

    def send(self, user_input, step_msg_func=None):
        clean_input = _squeeze_text(user_input)
        self.messages.append({"role": "user", "content": clean_input})

        step_msg = ""
        step_count = 0

        def step_msg_handler(msg):
            nonlocal step_msg
            if step_msg_func:
                step_msg_func(step_msg)
            step_msg += msg
            if step_msg_func:
                step_msg_func(step_msg)

        # æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡é™åˆ¶ï¼Œé¿å…æ— é™å¾ªç¯
        round_idx = 1
        while round_idx <= MAX_TOOL_ROUNDS:
            print(f"\n=== ğŸ”„ ç¬¬ {round_idx} è½® ===", flush=True)
            print("â€”â€” æ€è€ƒ / å›ç­”æµ â€”â€”", flush=True)

            # ä¸å†è‡ªåŠ¨ä¿®å‰ªä¸Šä¸‹æ–‡
            self._trim_context()

            # é¢å¤–æç¤ºï¼šè‹¥æ˜æ˜¾è¶…é™ï¼Œæå‰æ‰“å°æé†’ï¼ˆä¸è‡ªåŠ¨åˆ å‡ï¼‰
            current_tokens = self._count_tokens(self.messages)
            max_tokens = MAX_CONTEXT_LENGTH - TOKEN_BUFFER
            if current_tokens > max_tokens:
                print(f"âš ï¸ é¢„è®¡ä¸Šä¸‹æ–‡ {current_tokens} tokens å·²è¶…è¿‡æ¨¡å‹å»ºè®®ä¸Šé™ {max_tokens}ï¼Œ"
                      f"å¦‚å‡ºç°APIæŠ¥é”™è¯·æ‰‹åŠ¨å‡å°‘å†å²æˆ–æ¸…ç©ºå¯¹è¯ã€‚", flush=True)

            try:
                resp = self.instance.chat.completions.create(
                    model=self.model["name"],
                    messages=self.messages,
                    tools=self.tools,
                    **self.config["model_default_params"]
                ).choices[0]
            except Exception as e:
                # ç¦æ­¢è‡ªåŠ¨è£å‰ªï¼Œç›´æ¥æŠ›å‡ºå¸¦æŒ‡å¼•çš„é”™è¯¯
                if "maximum context length" in str(e).lower() or "context_length" in str(e).lower():
                    raise RuntimeError(
                        "ä¸Šä¸‹æ–‡è¶…è¿‡æ¨¡å‹æœ€å¤§é™åˆ¶ã€‚ä¸ºä¿è¯æ¶ˆæ¯å®Œæ•´æ€§ï¼Œå·²ç¦ç”¨è‡ªåŠ¨ä¿®å‰ªã€‚\n"
                        "è¯·æ¸…ç©ºå†å²æˆ–å‡å°‘è¾“å…¥åé‡è¯•ã€‚"
                    ) from e
                else:
                    raise

            resp_message = resp.message.to_dict()
            self.messages.append(resp_message)
            if resp.finish_reason == 'stop':
                print("â€”â€” æœ€ç»ˆç­”å¤ â€”â€”", flush=True)
                if resp_message.get("content"):
                    print(resp_message["content"], flush=True)
                resp_message["cards"] = [{"title": "ğŸ” è¯¦ç»†åˆ†æ", "content": step_msg, "expanded": False}]
                if step_msg_func:
                    step_msg_func(step_msg)
                return resp_message

            step_count += 1
            step_msg_handler("ç¬¬ %d æ­¥\n\n" % step_count)
            if resp_message.get("content"):
                step_msg_handler("å†…å®¹ï¼š%s\n\n" % resp_message["content"])
                print(resp_message["content"], end="", flush=True)

            if resp.message.tool_calls:
                print("\nâ€”â€” å·¥å…·è°ƒç”¨ â€”â€”", flush=True)
                for call in resp.message.tool_calls:
                    raw_args = call.function.arguments
                    args = self.safe_json_loads(raw_args)
                    func_name = call.function.name

                    step_msg_handler(
                        "å‡½æ•°æ‰§è¡Œå‡†å¤‡, å‡½æ•°å:%s, å‚æ•°:%s\n\n" % (call.function.name, call.function.arguments))
                    service_name = self._tool_to_service_map.get(func_name, "æœªçŸ¥æœåŠ¡")
                    print(f"â–¶ è°ƒç”¨ {func_name} [{service_name}] å‚æ•°: {json.dumps(args, ensure_ascii=False)}",
                          flush=True)

                    try:
                        result = self.funcDict[func_name](**args)
                    except Exception as e:
                        result = f"[å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}]\nè¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å‚æ•°æˆ–æ¢ä¸€ä¸ªå·¥å…·æ¥å®Œæˆä»»åŠ¡"

                    # å·¥å…·ç»“æœï¼šä»…åšè½»é‡å‹ç¼©ï¼Œä¸æˆªæ–­
                    content = json.dumps(result, ensure_ascii=False, default=_json_default)
                    if _is_json_like(content):
                        content = _minify_json_str(content)
                    else:
                        content = _squeeze_text(content)
                    self.messages.append({
                        "role": "tool",
                        "tool_call_id": call.id,
                        "name": func_name,
                        "content": content
                    })
                    step_msg_handler("å‡½æ•°æ‰§è¡Œç»“æœï¼š%s\n\n" % content)

            step_msg_handler("")
            round_idx += 1

        # å…œåº•è¿”å›
        print("â€”â€” æœ€ç»ˆç­”å¤ â€”â€”", flush=True)
        print("[è¶…å‡ºæœ€å¤§å·¥å…·è½®æ¬¡]", flush=True)
        fail_message = {"role": "assistant", "content": "[è¶…å‡ºæœ€å¤§å·¥å…·è½®æ¬¡]"}
        fail_message["cards"] = [{"title": "ğŸ” è¯¦ç»†åˆ†æ", "content": step_msg, "expanded": False}]
        self.messages.append(fail_message)
        if step_msg_func:
            step_msg_func(step_msg)
        return fail_message

    # ========== MCP å·¥å…·åŠ è½½ ==========
    def iter_package_modules(self, package_name: str) -> Iterator[ModuleType]:
        package = importlib.import_module(package_name)
        if not hasattr(package, '__path__'):
            raise ValueError(f"{package_name} is not a package")
        for importer, modname, ispkg in pkgutil.walk_packages(path=package.__path__,
                                                              prefix=package.__name__ + ".",
                                                              onerror=lambda x: None):
            try:
                module = importlib.import_module(modname)
                yield module
            except ImportError as e:
                print(f"æ— æ³•å¯¼å…¥æ¨¡å— {modname}: {e}")

    def get_module_global_functions(self, module: ModuleType) -> List[Tuple[str, callable]]:
        functions = inspect.getmembers(module, inspect.isfunction)
        return functions

    def extract_all_functions_from_package(self, package_name: str) -> dict:
        all_functions = {}
        for module in self.iter_package_modules(package_name):
            module_name = module.__name__
            functions = self.get_module_global_functions(module)
            if functions:
                all_functions[module_name] = functions
            all_functions[module_name] = functions
        return all_functions

    def get_mcp_tools(self):
        mcp_tools = []
        mcp_funcDict = {}

        servers = self.config.get("mcpServers", {})
        mcp_service_info = {}

        for name, cfg in servers.items():
            # è·³è¿‡è¢«ç¦ç”¨çš„æœåŠ¡
            if cfg.get("disabled", False):
                print(f"â­ï¸ è·³è¿‡å·²ç¦ç”¨æœåŠ¡: {name}", flush=True)
                continue

            cmd = cfg.get("command")
            args = cfg.get("args", [])
            cwd = cfg.get("workingDirectory") or cfg.get("working_directory") or os.getcwd()

            try:
                bridge = MCPBridge(command=cmd, args=args, cwd=cwd, timeout=30.0)
                self._mcp_bridges.append(bridge)

                # è¿œç¨‹å·¥å…·çš„ schema ç›´æ¥å¹¶å…¥
                tools = bridge.get_tools()

                # è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯
                service_info = bridge.get_service_info()

                # ä¸ºæ¯ä¸ªå·¥å…·æ·»åŠ æœåŠ¡ä¿¡æ¯åˆ°æè¿°ä¸­ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£å·¥å…·æ¥æº
                for tool in tools:
                    function_info = tool["function"]
                    original_description = function_info["description"]
                    function_info["description"] = f"[{name}æœåŠ¡] {original_description}"

                mcp_tools.extend(tools)

                mcp_service_info[name] = {
                    "tools": service_info["tools"],
                    "working_directory": service_info["service"]["working_directory"]
                }

                # è¿œç¨‹å·¥å…·çš„è°ƒç”¨åˆ†å‘å‡½æ•°ä¹Ÿå¹¶å…¥
                mcp_funcDict.update(bridge.get_func_map())

                print(f"MCP å·²å°±ç»ª: {name}", flush=True)
            except Exception as e:
                print(f"MCP å¯åŠ¨å¤±è´¥: {name} - {e}", flush=True)
                continue

        # ä¿å­˜æœåŠ¡ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        self._mcp_service_info = mcp_service_info
        # åˆ›å»ºå·¥å…·åˆ°æœåŠ¡çš„æ˜ å°„
        self._tool_to_service_map = {}
        for service_name, service_info in mcp_service_info.items():
            for tool_name in service_info["tools"]:
                self._tool_to_service_map[tool_name] = service_name

        print(f"å¯ç”¨å·¥å…·æ€»æ•°: {len(mcp_funcDict)}", flush=True)

        # ===== æœ¬åœ°æ‰¹é‡æ‰§è¡Œå·¥å…·ï¼ˆæŠŠå¤šæ¬¡åŒåè°ƒç”¨æ”¶æ•›åˆ°åŒä¸€è½®ï¼‰=====
        def _local_batch_exec(tool: str, args_list: list, mode: str = "sequential", max_concurrency: int = 4):
            """
            å¯¹åŒä¸€å·¥å…·è¿›è¡Œæ‰¹é‡è°ƒç”¨ï¼šæŠŠå¤šç»„å‚æ•°ä¸€æ¬¡æ€§æäº¤ï¼ŒæŒ‰é¡ºåºè¿”å›ç»“æœã€‚
            é»˜è®¤é¡ºåºæ‰§è¡Œï¼›å¦‚éœ€å¹¶å‘ï¼Œå¯è‡ªè¡Œæ‰©å±•ã€‚
            """
            if tool not in mcp_funcDict:
                return {"ok": False, "error": f"unknown tool '{tool}'"}
            fn = mcp_funcDict[tool]
            results = []
            for i, a in enumerate(args_list or []):
                try:
                    if not isinstance(a, dict):
                        raise ValueError("each args in args_list must be an object")
                    data = fn(**a)
                    results.append({"i": i, "ok": True, "data": data})
                except Exception as e:
                    results.append({"i": i, "ok": False, "error": str(e)})
            return {"ok": True, "results": results}

        batch_tool_schema = {
            "type": "function",
            "function": {
                "name": "batch_exec",
                "description": "å¯¹åŒä¸€å·¥å…·è¿›è¡Œæ‰¹é‡è°ƒç”¨ï¼šæä¾›ç›®æ ‡å·¥å…·å tool ä»¥åŠå‚æ•°æ•°ç»„ args_listï¼Œä¸€æ¬¡æ€§æ‰§è¡Œå¹¶æŒ‰è¾“å…¥é¡ºåºè¿”å›ç»“æœã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "tool": {"type": "string", "description": "è¦æ‰¹é‡è°ƒç”¨çš„å·¥å…·åï¼ˆå¿…é¡»æ˜¯å·²æœ‰å·¥å…·ï¼‰"},
                        "args_list": {
                            "type": "array",
                            "items": {"type": "object"},
                            "description": "æ¯æ¬¡è°ƒç”¨çš„ä¸€ç»„å‚æ•°å¯¹è±¡ï¼Œé¡ºåºå³æ‰§è¡Œé¡ºåº"
                        },
                        "mode": {"type": "string", "enum": ["sequential", "parallel"], "default": "sequential"},
                        "max_concurrency": {"type": "integer", "minimum": 1, "default": 4}
                    },
                    "required": ["tool", "args_list"]
                }
            }
        }

        mcp_tools.append(batch_tool_schema)
        mcp_funcDict["batch_exec"] = _local_batch_exec
        self._tool_to_service_map["batch_exec"] = "local-batch"

        return mcp_tools, mcp_funcDict

    def _shutdown_mcp(self):
        for b in getattr(self, "_mcp_bridges", []):
            try:
                b.stop()
            except Exception:
                pass

if __name__ == "__main__":
    try:
        client = Client()
        client.set_model(client.config["models"][0])

        print("è¿›å…¥é—®ç­”æ¨¡å¼ï¼ˆCtrl+C é€€å‡ºï¼‰", flush=True)
        while True:
            try:
                q = input("ä½ ï¼š").strip()
                if not q:
                    continue
                resp = client.send(q)
                print("\nåŠ©æ‰‹ï¼š", (resp.get("content") or "").strip(), "\n")
            except KeyboardInterrupt:
                print("\nå·²é€€å‡ºã€‚")
                break
            except Exception as e:
                print(f"é”™è¯¯ï¼š{e}")
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥ï¼š{e}")
